{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Import-and-Describe-the-Dataset\" data-toc-modified-id=\"Challenge-1---Import-and-Describe-the-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Import and Describe the Dataset</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?\" data-toc-modified-id=\"Explore-the-dataset-with-mathematical-and-visualization-techniques.-What-do-you-find?-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Explore the dataset with mathematical and visualization techniques. What do you find?</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Data-Cleaning-and-Transformation\" data-toc-modified-id=\"Challenge-2---Data-Cleaning-and-Transformation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Data Cleaning and Transformation</a></span></li><li><span><a href=\"#Challenge-3---Data-Preprocessing\" data-toc-modified-id=\"Challenge-3---Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Challenge 3 - Data Preprocessing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.\" data-toc-modified-id=\"We-will-use-the-StandardScaler-from-sklearn.preprocessing-and-scale-our-data.-Read-more-about-StandardScaler-here.-4.0.0.1\"><span class=\"toc-item-num\">4.0.0.1&nbsp;&nbsp;</span>We will use the <code>StandardScaler</code> from <code>sklearn.preprocessing</code> and scale our data. Read more about <code>StandardScaler</code> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\" target=\"_blank\">here</a>.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Data-Clustering-with-K-Means\" data-toc-modified-id=\"Challenge-4---Data-Clustering-with-K-Means-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 4 - Data Clustering with K-Means</a></span></li><li><span><a href=\"#Challenge-5---Data-Clustering-with-DBSCAN\" data-toc-modified-id=\"Challenge-5---Data-Clustering-with-DBSCAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 5 - Data Clustering with DBSCAN</a></span></li><li><span><a href=\"#Challenge-6---Compare-K-Means-with-DBSCAN\" data-toc-modified-id=\"Challenge-6---Compare-K-Means-with-DBSCAN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Challenge 6 - Compare K-Means with DBSCAN</a></span></li><li><span><a href=\"#Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters\" data-toc-modified-id=\"Bonus-Challenge-2---Changing-K-Means-Number-of-Clusters-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Bonus Challenge 2 - Changing K-Means Number of Clusters</a></span></li><li><span><a href=\"#Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples\" data-toc-modified-id=\"Bonus-Challenge-3---Changing-DBSCAN-eps-and-min_samples-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Bonus Challenge 3 - Changing DBSCAN <code>eps</code> and <code>min_samples</code></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings                                              \n",
    "from sklearn.exceptions import DataConversionWarning          \n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The origin of the dataset is [here](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data: Wholesale customers data\n",
    "data = pd.read_csv('../data/Wholesale customers data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.head())\n",
    "display(data.describe())\n",
    "display(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum columns to see the total of each\n",
    "data.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data['Channel'].value_counts())\n",
    "display(data['Region'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see columnwise distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "data.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check skewness and kurtosis\n",
    "display(data.skew())\n",
    "display(data.kurt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data follows the pareto principle\n",
    "total_spent = data.drop(columns=['Channel', 'Region']).sum(axis=1)\n",
    "total_spent.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "great_total = data.drop(columns=['Channel', 'Region']).sum().sum()\n",
    "\n",
    "cumulative = 0\n",
    "for i, value in enumerate(total_spent):\n",
    "    cumulative += value\n",
    "    if cumulative >= great_total * 0.8:\n",
    "        break\n",
    "\n",
    "print(f'The top {i+1} customers account for 80% of the total spent.')\n",
    "print(f'{i+1} is {((i+1)/len(data))*100:.2f}% of the total customers.')\n",
    "\n",
    "\n",
    "''' The pareto principle is not followed in this dataset.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_spend = total_spent.cumsum() / total_spent.sum()\n",
    "\n",
    "# Generates the pareto plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_spend.values, label=\"Cumulative Spend\", color=\"blue\")\n",
    "plt.axhline(y=0.8, color=\"red\", linestyle=\"--\", label=\"80% Threshold\")\n",
    "# plot vertical line at 20% of customers\n",
    "plt.axvline(x=len(data)*0.2, color=\"green\", linestyle=\"--\", label=\"20% Customers\")\n",
    "\n",
    "# plot i+i vertical line\n",
    "plt.axvline(x=i+1, color=\"orange\", linestyle=\":\", label=f\"Customers that account for 80% of the total spent\")\n",
    "\n",
    "# show label also on tick i+1\n",
    "# plt.xticks(list(plt.xticks()[0]) + [i+1])\n",
    "\n",
    "plt.title(\"Pareto Analysis: Cumulative Spend\")\n",
    "plt.xlabel(\"Customers (sorted by spending)\")\n",
    "plt.ylabel(\"Cumulative Spend Ratio\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "    * > 1)\tFRESH: annual spending (m.u.) on fresh products (Continuous);\n",
    "      > 2)\tMILK: annual spending (m.u.) on milk products (Continuous);\n",
    "      > 3)\tGROCERY: annual spending (m.u.)on grocery products (Continuous);\n",
    "      > 4)\tFROZEN: annual spending (m.u.)on frozen products (Continuous)\n",
    "      > 5)\tDETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous) \n",
    "      > 6)\tDELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous); \n",
    "      > 7)\tCHANNEL: customer Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)\n",
    "      > 8)\tREGION: customer Region - Lisnon, Oporto or Other (Nominal)\n",
    "\n",
    "* Any categorical data to convert?\n",
    "    * > CHANNEL and REGION\n",
    "\n",
    "* Any missing data to remove?\n",
    "    * > No missing data\n",
    "\n",
    "* Column collinearity - any high correlations?\n",
    "    * > Yes, DETERGENTS_PAPER and GROCERY\n",
    "\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "    * > Yes, there are outliers in all columns\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "    * > Yes, most columns are skewed to the right, and some columns have a lot of outliers. for kurtosis, most columns have a positive value. \n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "> - GROCERY and DETERGENTS_PAPER are highly correlated\n",
    "> - Channel and region are categorical data\n",
    "> - There is no missing data in the dataset\n",
    "> - The columns are skewed to the right\n",
    "> - There are outliers in all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix skewness\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "data['Fresh'] = boxcox(data['Fresh'])[0]\n",
    "data['Milk'] = boxcox(data['Milk'])[0]\n",
    "data['Grocery'] = boxcox(data['Grocery'])[0]\n",
    "data['Frozen'] = boxcox(data['Frozen'])[0]\n",
    "data['Detergents_Paper'] = boxcox(data['Detergents_Paper'])[0]\n",
    "data['Delicassen'] = boxcox(data['Delicassen'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('skewness after fixing:')\n",
    "data.skew()\n",
    "\n",
    "print('kurtosis after fixing:')\n",
    "data.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataset columns histograms\n",
    "plt.figure(figsize=(12, 8))\n",
    "data.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.kurtosis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr = data.corr()\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "data.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After fixing the skewness, the correlation between GROCERY and DETERGENTS_PAPER diminished. Therefore, there is no need to delete one of the columns.\n",
    "> from the boxplot we can see that there are outliers in all columns, but we will not remove them because they are not errors, they are just extreme values. There are not much of them, so we will keep them.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove one of the highly correlated columns\n",
    "# data.drop('Grocery', axis=1, inplace=True)\n",
    "\n",
    "# # Channel and Region are categorical variables, so we will convert them to dummy variables \n",
    "# data = pd.get_dummies(data, columns=['Channel', 'Region'], drop_first=True)\n",
    "# data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "-  ...\n",
    "-  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your import here:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code here:\n",
    "customers_scale = StandardScaler().fit_transform(data)\n",
    "print(customers_scale.shape)\n",
    "customers_scale.max(), customers_scale.min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2).fit(customers_scale)\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "data['Label'] = clusters \n",
    "data['Label'].value_counts()\n",
    "\n",
    "predicted_labels = km.predict(customers_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking to the elbow we can choose 2 like the correct number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was already written here...\n",
    "\n",
    "# kmeans_2 = KMeans(n_clusters=2).fit(customers_scale)\n",
    "\n",
    "# labels = kmeans_2.predict(customers_scale)\n",
    "\n",
    "# clusters = kmeans_2.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_customers['Label'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count values in predicted_labels\n",
    "print(np.bincount(predicted_labels))\n",
    "\n",
    "data['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN \n",
    "\n",
    "# Your code here\n",
    "dbscan = DBSCAN(eps=0.5).fit(customers_scale)\n",
    "data['labels_DBSCAN'] = dbscan.labels_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "data['labels_DBSCAN'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 6 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x,y,hue1, hue2):\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    plt.subplot(1,2,1) \n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue1)\n",
    "    plt.title(x.name + ' vs ' + y.name + '(' + hue1.name + ')')\n",
    "\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.scatterplot(x=x, \n",
    "                    y=y,\n",
    "                    hue=hue2)\n",
    "    plt.title(x.name + ' vs ' + y.name + ' (' + hue2.name + ')')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# plot(data['Detergents_Paper'], data['Milk'], data['Label'])\n",
    "plot(data['Detergents_Paper'], data['Milk'], data['Label'],data['labels_DBSCAN'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "plot(data['Grocery'], data['Fresh'], data['Label'], data['labels_DBSCAN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "plot(data['Frozen'], data['Delicassen'], data['Label'], data['labels_DBSCAN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "display(data.groupby(['Label']).mean())\n",
    "\n",
    "data.groupby(['labels_DBSCAN']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations here**\n",
    "\n",
    "> - K-means performed better than DBSCAN. DBSCAN set all the data as outliers, and K-means was able to cluster the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search kmeans clusters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import silhouette_score\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in n_clusters:\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(customers_scale)\n",
    "    # count values in np array\n",
    "    print('\\n',np.bincount(kmeans.labels_))    \n",
    "\n",
    "    print(f'Silhouette score for {i} clusters: {silhouette_score(customers_scale, kmeans.labels_)}')\n",
    "# param_grid = {'n_clusters': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
    "# kmeans = KMeans()\n",
    "# kmeans_cv = GridSearchCV(kmeans, param_grid, cv=3)\n",
    "# kmeans_cv.fit(customers_scale)\n",
    "# print(kmeans_cv.best_params_)\n",
    "# print(kmeans_cv.best_score_)\n",
    "\n",
    "print(\"The silhouette score represents how similar an object is to its own cluster (cohesion) compared to other clusters (separation). \\nIdeally, we want the silhouette score to be close to 1. The silhouette score is the highest when the number of \\nclusters is 2. This is because the data is not well separated into clusters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH for DBSCAN, changing eps and min_samples\n",
    "eps= [0.1, 0.5, 1, 1.5, 2] \n",
    "min_samples = [5, 10, 15, 20, 25]\n",
    "\n",
    "for i in eps:\n",
    "    for j in min_samples:\n",
    "        dbscan = DBSCAN(eps=i, min_samples=j)\n",
    "        dbscan.fit(customers_scale)\n",
    "        print(f'eps: {i}, min_samples: {j}')\n",
    "        print(pd.Series(dbscan.labels_).value_counts())\n",
    "        print()\n",
    "\n",
    "\n",
    "# dbscan = DBSCAN()\n",
    "# dbscan_cv = GridSearchCV(dbscan, param_grid, cv=3,scoring='adjusted_rand_score')\n",
    "# dbscan_cv.fit(customers_scale)\n",
    "# print(dbscan_cv.best_params_)\n",
    "# print(dbscan_cv.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comment here**\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
